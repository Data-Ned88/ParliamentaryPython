{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "53ed21e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from nltk import word_tokenize, ngrams,WordNetLemmatizer,PorterStemmer,sent_tokenize # had to download punkt from cmd line - import nltk nltk.download()\n",
    "from nltk.corpus import wordnet as wordn #show if the word is an adjective, noun, verb or adverb\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import datetime\n",
    "from textblob import TextBlob\n",
    "import statistics\n",
    "english_sw = stopwords.words('english') # same as comment above for stopwords corpus\n",
    "\n",
    "\n",
    "# stopwords have some useful stuff in when taken in the context of multi-grams\n",
    "english_sw_redact = english_sw\n",
    "\n",
    "for z in ['during','against','below','before', 'after', 'above','yourself', 'yourselves',\n",
    "          'ourselves','over', 'under', 'again', 'further','until', 'while']:\n",
    "    english_sw_redact.remove(z)\n",
    "    \n",
    "    \n",
    "extra_stop = ['us','really','would','make','get','say','also',\n",
    "              'made','many','take','over','under','going','let',\n",
    "              'put','may','see','give','back','every','given',\n",
    "              'done','go','before','two','next','across','says',\n",
    "              'come','could','per','since','however','after','much',\n",
    "              'again','able','whether','taken','cent','even','extra','mr','set','still','ago','thing','asked','got',\n",
    "              'last','hour','one','two','three','four','five','six','seven','eight','nine','ten',\n",
    "              'monday','tuesday','wednesday','thursday','friday','saturday','sunday','day','today','yesterday',\n",
    "              'january','february','march','april','june','july','august','september','october','november','december'\n",
    "]    \n",
    "english_sw.extend(extra_stop)\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "ps = PorterStemmer()\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "loca = \"\\\\Analysis\\\\\" # where to save the processed NGrams files.\n",
    "_pmqmaster = \"ALLCONTRIBSMASTER -CAT.txt\" # Raw Transcript log file from Hansard API\n",
    "\n",
    "donedata = []\n",
    "with open(loca + 'AllDataPBI.txt','r') as latestfl:\n",
    "    latestfl.readline()\n",
    "    lrdr = csv.reader(latestfl,delimiter='\\t')\n",
    "    for lrow in lrdr:\n",
    "        donedata.append(lrow)\n",
    "\n",
    "donemonths = [d[2] for d in donedata]\n",
    "donemonths = list(set(donemonths))\n",
    "donemonths = [datetime.datetime.strptime(d,\"%Y-%m-%d\") for d in donemonths]\n",
    "lastmonth = max(donemonths)\n",
    "lastmonthtext = datetime.datetime.strftime(lastmonth,\"%Y-%m-%d\")\n",
    "\n",
    "donedata = [d for d in donedata if d[2] != lastmonthtext] # lop off last month of done data\n",
    "donemonths = [d for d in donemonths if d != lastmonth]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab789c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2023, 7, 1, 0, 0)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lastmonth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdcf7a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "531735a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed raw data into list var\n",
      "Starting ngrams\n",
      "All 1 ngrams:  7231\n",
      "Dict done\n",
      "Disallow done 0\n",
      "All 2 ngrams:  3688\n",
      "Dict done\n",
      "Disallow done 0\n",
      "All 3 ngrams:  1313\n",
      "Dict done\n",
      "Disallow done 0\n",
      "All 4 ngrams:  4253\n",
      "Dict done\n",
      "Disallow done 0\n",
      "All 5 ngrams:  7490\n",
      "Dict done\n",
      "Disallow done 0\n",
      "All 6 ngrams:  3785\n",
      "Dict done\n",
      "Disallow done 0\n",
      "All 7 ngrams:  1675\n",
      "Dict done\n",
      "Disallow done 0\n",
      "Starting Tenure Stats and WOrd COunts\n",
      "Filemerge\n",
      "Appending done data\n",
      "Sentiment\n"
     ]
    }
   ],
   "source": [
    "conslist = []\n",
    "rejects = []\n",
    "regs = ['\\[[\\w\\W\\s]{1,25}\\]','\\([\\w\\W\\s]{1,25}\\)','^rose\\u2014','^rose\\-','^rose$']\n",
    "exceptions = ['(a) are in prison and (b)','(a) disposal, (b)']\n",
    "with open(_pmqmaster,'r') as fl:\n",
    "    line = fl.readline()\n",
    "    line = fl.readline()\n",
    "    cnt = 1\n",
    "    while line:\n",
    "        \n",
    "        rej = []\n",
    "        \n",
    "        lal = line.strip().split('\\t')\n",
    "        rowlist = [cnt]\n",
    "        rowlist.extend(lal[3:5])\n",
    "        yrmonth = datetime.datetime.strptime(lal[4],'%d/%m/%Y')\n",
    "        rowlist.append(datetime.datetime(yrmonth.year,yrmonth.month,1))#monthyear\n",
    "        rowlist.extend(lal[5:])\n",
    "        #conslist.append(rowlist)\n",
    "        spc = lal[3]\n",
    "        for rx in regs:\n",
    "            res = re.findall(rx,spc)\n",
    "            rej.extend(res)\n",
    "        if len(rej) > 0:\n",
    "            #rejects.append([cnt,rej])\n",
    "            rejunique = [x for x in list(set(rej)) if x not in exceptions]\n",
    "            for rejterm in rejunique:\n",
    "                spc = spc.replace(rejterm,'')\n",
    "        if re.search('\\w',spc):\n",
    "            rowlist[1] = spc\n",
    "            conslist.append(rowlist)\n",
    "        else:\n",
    "            rejects.append(rowlist)\n",
    "        cnt += 1\n",
    "        line = fl.readline()\n",
    "\n",
    "\n",
    "        \n",
    "print(\"Processed raw data into list var\")      \n",
    "\n",
    "def tokensprep(tst):\n",
    "    #tst is a list of words\n",
    "    #connect spoken lang orphans (haven't), hon. as in honourable, and replace curly format double-quotations\n",
    "    orphans = [\"'s\",\"'t\", \"'ve\", \"'ll\", \"'re\",\"'d\"]\n",
    "    for i,v in enumerate(tst):\n",
    "        for o in orphans:\n",
    "            if o in v.lower():\n",
    "        \n",
    "                tst[i-1] = tst[i-1] + v\n",
    "                tst.remove(tst[i])\n",
    "                break\n",
    "                \n",
    "    for i,v in enumerate(tst):\n",
    "        if tst[i] == '.' and tst[i-1].lower() == 'hon':\n",
    "            tst[i-1] = tst[i-1] + v\n",
    "            tst.remove(tst[i])\n",
    "    \n",
    "    \n",
    "    for t in tst:\n",
    "        t = t.replace('\\u201d','\"').replace('\\u201c','\"')\n",
    "    \n",
    "    return tst\n",
    "        \n",
    "def ngrams_useful(x,y,bigram_switch=False):\n",
    "    #return True if the ngram (as list var x) has no punctuation as one of the words, and has less than ...\n",
    "    #... var y number of REDACTED stopwords. Should stop 'I am' appearing as bigram\n",
    "    check = True\n",
    "    ct = 0\n",
    "    # flag invalid if punctuation\n",
    "    for wrd in x:\n",
    "        if not re.search('\\w',wrd):\n",
    "            check = False\n",
    "            break\n",
    "    #build stopword score\n",
    "    for wrd in x:\n",
    "        if wrd.lower() in english_sw_redact:\n",
    "            ct +=1\n",
    "            \n",
    "    if ct >= y:\n",
    "        check=False\n",
    "        \n",
    "    if bigram_switch: #if running on bigrams, both components can't be in the extended stopwords, and no numbers \n",
    "        ctb = 0\n",
    "        for wrd in x:\n",
    "            if wrd.lower() in english_sw:\n",
    "                ctb +=1\n",
    "            elif re.search('[0-9]',wrd.lower()):\n",
    "                ctb +=2\n",
    "        if ctb >=2:\n",
    "            check=False\n",
    "    return check        \n",
    "        \n",
    "def smart_lemma(x):\n",
    "    try:\n",
    "        wordtype = wordn.synsets(x)[0].pos() # get the first pos tag of the word. 'a' and 's' are adjectives,'v'=verb,'r'=adverb,'n'=noun\n",
    "        if wordtype == 'n':\n",
    "            out = lemmatizer.lemmatize(x) #noun is default for lemmatizer. THis should standardise plurals\n",
    "        elif wordtype == 'v':\n",
    "            out = lemmatizer.lemmatize(x, pos='v') #standardize verbs\n",
    "        else:\n",
    "            #leave alone\n",
    "            out = x\n",
    "    except:\n",
    "        out = x\n",
    "    return out        \n",
    "        \n",
    "def ngramlist(x,y,swmax,bgswitch=False):\n",
    "    #return list of distinct ngrams with counts, where x is a list of speeches, y is n grams, and swmax is the limit...\n",
    "    #... to the number of REDACTED stopwords that can appear in the gram for it to be counted and returned.\n",
    "    #bgswitch tells the function that it is running on bigrams - so the ngrams_useful call should factor this in and check\n",
    "    #...both words in bigrams against the extendend stopwords\n",
    "    gramslist = []\n",
    "    for spch in x:\n",
    "        \n",
    "        if y > 1:\n",
    "            toks = tokensprep(list(word_tokenize(spch)))\n",
    "            rawgrams = [list(b) for b in list(ngrams(toks,y))]\n",
    "            cleangrams = [re.sub('\\.$','',\" \".join(c).lower()) for c in rawgrams if ngrams_useful(c,swmax,bgswitch)]\n",
    "            gramslist.extend(cleangrams)\n",
    "        else:\n",
    "            toks = tokensprep(list(word_tokenize(spch.replace('…',' '))))\n",
    "            cleangrams = [re.sub('\\.$','',tko.lower()) for tko in toks if not re.search('[^a-z]', tko.lower()) and tko.lower() not in english_sw]\n",
    "            if len(cleangrams) > 0:\n",
    "                cleangramslemma = [smart_lemma(cl) for cl in cleangrams]\n",
    "                gramslist.extend(cleangramslemma)\n",
    "        \n",
    "    ngram_cts = Counter(gramslist)\n",
    "    return [[i[0],i[1]] for i in ngram_cts.items()]\n",
    "\n",
    "#WordCount\n",
    "def wordcount(x):\n",
    "    gramslist = []\n",
    "    for spch in x:\n",
    "        toks = tokensprep(list(word_tokenize(spch.replace('…',' '))))\n",
    "        cleangrams = [re.sub('\\.$','',tko.lower()) for tko in toks if re.search('\\w',tko)]\n",
    "        gramslist.extend(cleangrams)\n",
    "    return (len(gramslist),len(x))\n",
    "\n",
    "\n",
    "rejects = None\n",
    "\n",
    "months = list(set([x[3] for x in conslist if x[3] not in donemonths]))\n",
    "monthscts = list(set([x[3] for x in conslist]))\n",
    "roles = list(set([x[4] for x in conslist]))\n",
    "parties = list(set([x[5] for x in conslist]))\n",
    "people = list(set([x[-1] for x in conslist if x[4] != 'BB' and len(x) ==7]))\n",
    "\n",
    "print(\"Starting ngrams\")\n",
    "\n",
    "configsettings = [[1,1,1,'SingleIII.txt'],[2,1,1,'BGIII.txt'],[3,1,1,'TrigramsII.txt'],\n",
    "                  [4,2,1,'4_grams.txt'],[5,3,1,'5_grams.txt'],[6,3,1,'6_grams.txt'],[7,3,1,'7_grams.txt']]\n",
    "\n",
    "\n",
    "for c_st in configsettings:\n",
    "    masterlist = []\n",
    "    _n = c_st[0]\n",
    "    _swmax = c_st[1]\n",
    "    min_occ = c_st[2]\n",
    "    if _n==2: #propagate into ngrams checking\n",
    "        masterbgswitch = True\n",
    "    else:\n",
    "        masterbgswitch = False\n",
    "    for mo in months:\n",
    "        \n",
    "        analytime = [x for x in conslist if x[3] == mo]\n",
    "        for ro in roles:\n",
    "            analyroletime = [x for x in analytime if x[4] == ro]\n",
    "            for p in parties:\n",
    "                analyroletimepty = [x for x in analyroletime if x[5] == p]\n",
    "                inds = []\n",
    "                gen_bbs = []\n",
    "                for aaa in analyroletimepty:\n",
    "                    \n",
    "                    if ro != 'BB':\n",
    "                        inds.append(aaa)\n",
    "                    elif len(aaa)==7:\n",
    "                        if aaa[6] in people:\n",
    "                            \n",
    "                            inds.append(aaa)\n",
    "                        else:\n",
    "                            gen_bbs.append(aaa[1])\n",
    "                    else:\n",
    "                        gen_bbs.append(aaa[1])\n",
    "                        \n",
    "                #now have gen_bbs (can be done at once) and inds(needs to be looped thru person by person)\n",
    "                    \n",
    "                #operate function on gen_bbs\n",
    "                if len(gen_bbs)> 0:\n",
    "                    outgr_data_bbs = ngramlist(gen_bbs,_n,_swmax,masterbgswitch)\n",
    "                    if len(outgr_data_bbs) > 0:\n",
    "                        with_cats_bbs = [[x[0],x[1],mo,ro,p,''] for x in outgr_data_bbs]\n",
    "                        masterlist.extend(with_cats_bbs)\n",
    "                    \n",
    "                #for each person...\n",
    "                for ppl in people:\n",
    "                    #testlist = [x[2:] for x in inds if x[6] == ppl]\n",
    "                    #if len(testlist) > 0:\n",
    "                    #    for t in testlist:\n",
    "                    #    \n",
    "                    #        masterlist.append(t)\n",
    "                    personspecific = [x[1] for x in inds if x[6] == ppl]\n",
    "                    #operate on person specific\n",
    "                    if len(personspecific) > 0: #there is data for this person in this orle at this month\n",
    "                        #print(personspecific)\n",
    "                        outgr_data = ngramlist(personspecific,_n,_swmax,masterbgswitch)\n",
    "                        \n",
    "                        if len(outgr_data) > 0:\n",
    "                            \n",
    "                            with_cats = [[x[0],x[1],mo,ro,p,ppl] for x in outgr_data]\n",
    "                            masterlist.extend(with_cats)\n",
    "                    \n",
    "    \n",
    "    print('All {0} ngrams: '.format(_n), len(masterlist))                \n",
    "    #remove any that are single across whole file                \n",
    "    masterlistcts = {}\n",
    "    for m in masterlist:\n",
    "        trm = m[0]\n",
    "        ct = m[1]\n",
    "        if trm in masterlistcts:\n",
    "            masterlistcts[trm] += ct\n",
    "        else:\n",
    "            masterlistcts[trm] = ct\n",
    "    print('Dict done')        \n",
    "    disallowlist = [[x,1] for x in masterlistcts if masterlistcts[x] < min_occ]\n",
    "    print('Disallow done',len(disallowlist))\n",
    "    masterdf = pd.DataFrame(masterlist, columns=['Value', 'Count','YearMonth', 'Role','Party','Person'])\n",
    "    disallowdf = pd.DataFrame(disallowlist, columns=['ValueDis','Disallow'])\n",
    "    mergedf = pd.merge(masterdf, disallowdf, left_on='Value', right_on='ValueDis',how='left',validate='m:1')\n",
    "    neatdf = mergedf[mergedf['Disallow'] != 1].drop(['ValueDis','Disallow'],axis=1)\n",
    "    loca = \"C:\\\\Users\\\\nedst\\\\OneDrive\\\\Documents\\\\DataplusVisualPresentationII - LBAG\\\\UCOVI\\\\BLOG WORK\\\\Covid Post III - Parliament\\\\Debate Scrapes\\\\Analysis\\\\\"\n",
    "    neatdf.to_csv(loca + c_st[3],index=False,sep='\\t')\n",
    "\n",
    "\n",
    "print(\"Starting Tenure Stats and WOrd COunts\")    \n",
    "    \n",
    "masterlist = []\n",
    "\n",
    "for mo in monthscts:\n",
    "    #print(mo)\n",
    "    analytime = [x for x in conslist if x[3] == mo]\n",
    "    for ro in roles:\n",
    "        analyroletime = [x for x in analytime if x[4] == ro]\n",
    "        for p in parties:\n",
    "            analyroletimepty = [x for x in analyroletime if x[5] == p]\n",
    "            inds = []\n",
    "            gen_bbs = []\n",
    "            for aaa in analyroletimepty:\n",
    "                \n",
    "                if ro != 'BB':\n",
    "                    inds.append(aaa)\n",
    "                elif len(aaa)==7:\n",
    "                    if aaa[6] in people:\n",
    "                        \n",
    "                        inds.append(aaa)\n",
    "                    else:\n",
    "                        gen_bbs.append(aaa[1])\n",
    "                else:\n",
    "                    gen_bbs.append(aaa[1])\n",
    "                    \n",
    "            #now have gen_bbs (can be done at once) and inds(needs to be looped thru person by person)\n",
    "                \n",
    "            #operate function on gen_bbs\n",
    "            if len(gen_bbs)> 0:\n",
    "                wc,fre = wordcount(gen_bbs)\n",
    "                \n",
    "                with_cats_bbs = [wc,fre,None,mo,ro,p,'']\n",
    "                masterlist.append(with_cats_bbs)\n",
    "                \n",
    "            #for each person...\n",
    "            for ppl in people:\n",
    "                #testlist = [x[2:] for x in inds if x[6] == ppl]\n",
    "                #if len(testlist) > 0:\n",
    "                #    for t in testlist:\n",
    "                #    \n",
    "                #        masterlist.append(t)\n",
    "                personspecific = [x[1] for x in inds if x[6] == ppl]\n",
    "                persondates = len(list(set([x[2] for x in inds if x[6] == ppl])))\n",
    "                #operate on person specific\n",
    "                if len(personspecific) > 0: #there is data for this person in this orle at this month\n",
    "                    wcp,frep = wordcount(personspecific)\n",
    "\n",
    "                    with_cats = [wcp,frep,persondates,mo,ro,p,ppl]\n",
    "                    masterlist.append(with_cats)    \n",
    "\n",
    "masterdf = pd.DataFrame(masterlist, columns=['Value', 'Count','DebatesSpokenIn','YearMonth', 'Role','Party','Person'])\n",
    "masterdf.to_csv(loca + 'SpeechesAndWordCounts.txt',index=False,sep='\\t')\n",
    "\n",
    "tenures = []\n",
    "for mo in monthscts:\n",
    "    dates = list(set([x[2] for x in conslist if x[3] == mo]))\n",
    "    for d in dates:\n",
    "        distroles = list(set([x[4] for x in conslist if x[2] == d]))\n",
    "        \n",
    "        if 'Deputy Speaker' in distroles:\n",
    "            dsflag = 1\n",
    "        else:\n",
    "            dsflag = 0\n",
    "        if 'Dep PM (replacing)' in distroles:\n",
    "            dpflag = 1\n",
    "        else:\n",
    "            dpflag = 0 \n",
    "        \n",
    "        if 'Dep LOE (replacing)' in distroles:\n",
    "            dleflag = 1\n",
    "        else:\n",
    "            dleflag = 0             \n",
    "        tenures.append([mo,d,dsflag,dpflag,dleflag])\n",
    "        \n",
    "maxtenuresdate = max([datetime.datetime.strptime(x[1],'%d/%m/%Y') for x in tenures])\n",
    "\n",
    "\n",
    "pms = ['Tony Blair','Gordon Brown','David Cameron','Theresa May','Boris Johnson','Elizabeth Truss','Rishi Sunak']\n",
    "loes = ['John Major','William Hague','Iain Duncan Smith','Michael Howard','David Cameron','Ed Miliband',\n",
    "        'Jeremy Corbyn','Kier Starmer']\n",
    "\n",
    "speakers_min_max = [['Betty Boothroyd', datetime.datetime.strptime('21/05/1997','%d/%m/%Y'),datetime.datetime.strptime('26/07/2000','%d/%m/%Y')],\n",
    "                    ['Michael Martin',  datetime.datetime.strptime('25/10/2000','%d/%m/%Y'),datetime.datetime.strptime('17/06/2009','%d/%m/%Y')],\n",
    "                    ['John Bercow',     datetime.datetime.strptime('24/06/2009','%d/%m/%Y'),datetime.datetime.strptime('30/10/2019','%d/%m/%Y')],\n",
    "                    ['Lindsay Hoyle',   datetime.datetime.strptime('08/01/2020','%d/%m/%Y'),maxtenuresdate]]\n",
    "pms_min_max = []\n",
    "loes_min_max = []\n",
    "conslistnames = [x for x in conslist if len(x) == 7]\n",
    "for pm in pms:\n",
    "    dates = [datetime.datetime.strptime(x[2],'%d/%m/%Y') for x in conslistnames if x[4] == 'PM' and x[6] == pm]\n",
    "    pms_min_max.append([pm,min(dates),\n",
    "                        max(dates)])  \n",
    "for lo in loes:\n",
    "    dates = [datetime.datetime.strptime(x[2],'%d/%m/%Y') for x in conslistnames if x[4] == 'LOE' and x[6] == lo]\n",
    "    loes_min_max.append([lo,min(dates),\n",
    "                        max(dates)])\n",
    "    \n",
    "for te in tenures:\n",
    "    sitpm = None\n",
    "    sitloe = None\n",
    "    sitsp = None\n",
    "    dt = datetime.datetime.strptime(te[1],'%d/%m/%Y')\n",
    "    for p in pms_min_max:\n",
    "        if dt >= p[1] and dt <= p[2]:\n",
    "            sitpm = p[0]\n",
    "            break\n",
    "    for p in loes_min_max:\n",
    "        if dt >= p[1] and dt <= p[2]:\n",
    "            sitloe = p[0]\n",
    "            break\n",
    "    for p in speakers_min_max:\n",
    "        if dt >= p[1] and dt <= p[2]:\n",
    "            sitsp = p[0]\n",
    "            break \n",
    "                \n",
    "    # current leaders of opposition\n",
    "    if sitloe is None and te[0].year in [2021,2022,2023]:\n",
    "        sitloe = 'Kier Starmer'\n",
    "    if sitpm is None and te[0].year in [2022,2023]:\n",
    "        sitpm = 'Rishi Sunak'\n",
    "    \n",
    "    #\n",
    "    \n",
    "    \n",
    "    te.append(sitsp)\n",
    "    te.append(sitpm)\n",
    "    te.append(sitloe)\n",
    "    \n",
    "masterdf = pd.DataFrame(tenures, columns=['Month', 'Date',\n",
    "                                          'DepSpeaker','DepPM','DepLOE', \n",
    "                                          'SittingSpeaker','SittingPM','SittingLOE',])\n",
    "\n",
    "masterdf.to_csv(loca + 'DebateInfo.txt',index=False,sep='\\t')\n",
    "\n",
    "\n",
    "print(\"Filemerge\")\n",
    "\n",
    "\n",
    "filestom = [\n",
    " ['4_grams.txt','4-Grams'],\n",
    " ['5_grams.txt','5-Grams'],\n",
    " ['6_grams.txt','6-Grams'],\n",
    " ['7_grams.txt','7-Grams'],\n",
    " ['BGIII.txt','2.Bigrams'],\n",
    " ['SingleIII.txt','1.SingleWord'],\n",
    " ['TrigramsII.txt','3.Trigrams']\n",
    "]\n",
    "\n",
    "mergeurl = loca + 'AllDataPBI.txt'\n",
    "\n",
    "\n",
    "with open(mergeurl,\"w\",newline='') as mu:\n",
    "    wrt = csv.writer(mu,delimiter='\\t')\n",
    "    wrt.writerow(['Value','Count','YearMonth','Role','Party','Person','TextType'])\n",
    "    \n",
    "    for v in filestom:\n",
    "        textype = v[1]\n",
    "        with open(loca + v[0],\"r\") as raw:\n",
    "            \n",
    "            \n",
    "            raw.readline()\n",
    "            rdr = csv.reader(raw,delimiter='\\t')\n",
    "            for rw in rdr:\n",
    "                rw.append(textype)\n",
    "                wrt.writerow(rw)\n",
    "                \n",
    "print(\"Appending done data\")\n",
    "with open(mergeurl,\"a\",newline='') as mut:\n",
    "    wrtt = csv.writer(mut,delimiter='\\t')\n",
    "\n",
    "    \n",
    "    for rwt in donedata:\n",
    "\n",
    "        wrtt.writerow(rwt)            \n",
    "                \n",
    "print(\"Sentiment\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "81630d3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000\n",
      "6000\n",
      "9000\n",
      "12000\n",
      "15000\n",
      "18000\n",
      "21000\n",
      "24000\n",
      "27000\n",
      "30000\n",
      "33000\n",
      "36000\n",
      "39000\n",
      "42000\n",
      "45000\n",
      "48000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def full_senti_calc(x):\n",
    "    \n",
    "    sentences = sent_tokenize(x)\n",
    "    if len(sentences) == 0:\n",
    "        return {'TextBlob':None,'NltkVader':None}\n",
    "    tbscorelist = []\n",
    "    vadscorelist = []\n",
    "    for s in sentences:\n",
    "        wc = len(word_tokenize(s))\n",
    "        weight = 1.0 + ((float(wc) ** float(1/3)) * 0.1) #add 10% of cube root of word length as weight\n",
    "        tbscore = TextBlob(s).sentiment.polarity\n",
    "        vader_score = sia.polarity_scores(s)['compound']\n",
    "        tbscorelist.append(round(tbscore*weight,4))\n",
    "        vadscorelist.append(round(vader_score*weight,4))\n",
    "        \n",
    "        #print(s,'\\n','TextBlob: ',round(tbscore,4),'NLTK Vader: ',round(vader_score,4),'  ', weight)\n",
    "    return {'TextBlob':round(statistics.mean(tbscorelist),4),\n",
    "            'NltkVader':round(statistics.mean(vadscorelist),4)}\n",
    "\n",
    "def issue_tag(x,xdate,xtaglist):\n",
    "    tags = []\n",
    "    spacerep = re.sub('[\\.,:;\\?!\"]',' ',x).lower()\n",
    "    for trm in xtaglist:\n",
    "        if trm[0] in spacerep and xdate.year >= trm[2]:\n",
    "            tags.append(trm[1])\n",
    "    return sorted(list(set(tags)))\n",
    "\n",
    "tagsfile = loca + 'TaggingPhrases.txt'\n",
    "tagslist = []\n",
    "tagslistclean = []\n",
    "with open(tagsfile,'r') as tgfl:\n",
    "    line = tgfl.readline()\n",
    "    line = tgfl.readline()\n",
    "    while line:\n",
    "        rw = line.strip().split('\\t')\n",
    "        tagslist.append(rw)\n",
    "        line = tgfl.readline()\n",
    "\n",
    "\n",
    "for t in tagslist:\n",
    "    tspace = ' ' + t[0] + ' '\n",
    "    reg = tspace.replace('* ','')\n",
    "    tag = t[1]\n",
    "    if len(t) == 2:\n",
    "        year = 1996\n",
    "    else:\n",
    "        year = int(t[2])\n",
    "        \n",
    "    tagslistclean.append([reg,tag,year])\n",
    "    \n",
    "    \n",
    "conslist_light = []\n",
    "conslist_tags = []\n",
    "conslist_tagged = []\n",
    "rollct = 0\n",
    "for c in conslist:\n",
    "    rollct += 1\n",
    "    if rollct % 3000 == 0:\n",
    "        print(rollct)\n",
    "    _id = c[0]\n",
    "    _text = c[1]\n",
    "    _day = c[2]\n",
    "    _month = c[3]\n",
    "    _role,_party = c[4],c[5]\n",
    "    if len(c) == 7:\n",
    "        _person = c[6]\n",
    "    else:\n",
    "        _person = None\n",
    "    issues = issue_tag(_text,_month,tagslistclean)\n",
    "    _sentiments = full_senti_calc(_text)\n",
    "    \n",
    "    conslist_light.append([_id,_day,_month,_role,_party,_person\n",
    "\n",
    "                           ,_sentiments['TextBlob'],_sentiments['NltkVader']])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    if len(issues) > 0:\n",
    "        conslist_tags.extend([[_id,x] for x in issues])\n",
    "        conslist_tagged.append([_id,_month,_role,_party,_person\n",
    "                               ,_text])  \n",
    "        \n",
    "with open(loca + 'ContributionsSentiment.txt','w',newline='') as wr:\n",
    "    wrt = csv.writer(wr,delimiter='\\t')\n",
    "    wrt.writerow(['ID','DebateDate','DebateMonth','Role','Party','Person'\n",
    "                  #,'Text'])\n",
    "                  ,'TextBlob','NltkVader'])\n",
    "    for x in conslist_light:\n",
    "        wrt.writerow(x)\n",
    "        \n",
    "with open(loca + 'Tags.txt','w',newline='') as wr:\n",
    "    wrt = csv.writer(wr,delimiter='\\t')\n",
    "    wrt.writerow(['ConstributionID','Tag'])\n",
    "    for x in conslist_tags:\n",
    "        wrt.writerow(x)    \n",
    "        \n",
    "        \n",
    "with open(loca + 'TaggedContributionsSentiment.txt','w',newline='') as wr:\n",
    "    wrt = csv.writer(wr,delimiter='\\t')\n",
    "    wrt.writerow(['ID','DebateMonth','Role','Party','Person'\n",
    "                  ,'Text'])\n",
    "\n",
    "    for x in conslist_tagged:\n",
    "        wrt.writerow(x)       \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52858e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "conslist_light = None\n",
    "conslist_tags = None\n",
    "conslist_tagged = None\n",
    "conslist = None\n",
    "masterlist = None\n",
    "sentence_word_counts = None\n",
    "donedata = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2f4ea9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
